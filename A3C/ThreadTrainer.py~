from threading import Thread
import numpy as np
import threading

from Config import Config


class ThreadTrainer(Thread):
    def __init__(self, server, id):
        super(ThreadTrainer, self).__init__()
        self.setDaemon(True)
        self.id = id
        self.server = server
        self.exit_flag = False

    def run(self):
        print("trainer is main_thread ", threading.current_thread() == threading.main_thread())
        while not self.exit_flag:
            batch_size = 0
            while batch_size <= Config.TRAINING_MIN_BATCH_SIZE:
                # if not self.server.training_q.empty():
                x_, r_ = self.server.training_q.get()
                # else:
                #     self.exit_flag = True
                #     break
                if batch_size == 0:
                    x__ = x_
                    r__ = r_
                else:
                    x__ = np.concatenate((x__, x_))
                    r__ = np.concatenate((r__, r_))
                    # a__ = np.concatenate((a__, a_))
                # batch_size += x_.shape[0]
                batch_size += 1
                print("state: ", x__)
                print("value: ", r__)
                self.exit_flag = True
            # print("hello from thread ", self.id)
            # print("exit", self.exit_flag)
            # print(x__)
