import numpy as np
import tensorflow as tf
from numpy import linalg as LA
import random

random.seed(0)
np.random.seed(0)
seq_max_len = 10
lr = 1e-3
total_episodes = 10
batch_size = 5
num_units = 10


class Env:
    def __init__(self):
        self.total_reward = 0
        self.sample = []

    def generate_sample(self):
        self.sample = [np.random.normal(0, 1, 2) for x in range(seq_max_len)]

    def step(self, actions):
        for i in range(len(actions)-1):
            self.total_reward += LA.norm(self.sample[actions[i]] - self.sample[actions[i+1]])


class Data:
    def __init__(self, batch_size, seq_max_len):
        self.batch_size = batch_size
        self.seq_max_len = seq_max_len

    def generate_batch(self):
        s = []
        A = []
        G = []
        for i in range(self.batch_size):
            actions = [i for i in range(self.seq_max_len)]
            random.shuffle(actions)
            env = Env()
            env.generate_sample()
            env.step(actions)
            s.append(env.sample)
            A.append(actions)
            G.append(env.total_reward)
        return np.asarray(s), np.asarray(A), np.asarray(G)


data = Data(batch_size, seq_max_len)
s_batch, A_batch, G_batch = data.generate_batch()
print(s_batch.shape)
print(s_batch)
print()
tmp = s_batch.reshape((seq_max_len, batch_size, -1))


s = tf.placeholder(tf.float32, [None, seq_max_len, 1])
A = tf.placeholder(tf.float32, [None, seq_max_len, 1])
G = tf.placeholder(tf.float32, [None, 1])
seqlen = tf.placeholder(tf.int32, [None])

encoder_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units)
encoder_outputs, encoder_state = tf.nn.dynamic_rnn(
    encoder_cell,
    s,
    sequence_length=np.repeat([seq_max_len], batch_size),
    time_major=False,
    dtype=tf.float32)            # Not sure about time major

# decoder_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units)

with tf.Session() as sess:
    feed_dict = {s: s_batch, A: A_batch, G: G_batch}
    sess.run(encoder_outputs, feed_dict=feed_dict)


# s_in = tf.unstack(s, seq_max_len, 1)
# lstm_cell = tf.contrib.rnn.BasicLSTMCell(10)
# rnn_outputs, states = tf.contrib.rnn.static_rnn(lstm_cell, s_in, dtype=tf.float32, sequence_length=seqlen)
# output = tf.layers.dense(inputs=rnn_outputs, units=10, activation=None)

# neg_log_prob = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=output, labels=A_t)
# loss = tf.reduce_mean(neg_log_prob * G_t)

# trainable_variables = tf.trainable_variables()
# gradient_holders = []

# for idx, var in enumerate(trainable_variables):
#     placeholder = tf.placeholder(tf.float32, name=str(idx)+'_holder')
#     gradient_holders.append(placeholder)

# gradients = tf.gradients(loss, trainable_variables)

# optimizer = tf.train.AdamOptimizer(lr)
# update_batch = optimizer.apply_gradients(zip(gradient_holders, trainable_variables))

# init = tf.global_variables_initializer()

# with tf.Session() as sess:
#     sess.run(init)

#     i = 0
#     gradBuffer = sess.run(tf.trainable_variables())
#     for ix, grad in enumerate(gradBuffer):
#         gradBuffer[ix] = grad*0

#     while i < total_episodes:
#         i += 1
#         # s_in = np.random.uniform(0, 1, N).reshape(-1, 1)
#         # G_t, A_t = reward_to_end(s_in, t)
#         # # print(A_t, G_t)
#         # s_in = np.array([s_in[t]], dtype=np.float32)
#         # r_in = np.array([G_t], dtype=np.float32)
#         # a_in = np.array([A_t], dtype=np.int32)
#         # feed_dict = {s: s_in, reward_holder: r_in, action_holder: a_in}
#         # grads = sess.run(gradients, feed_dict=feed_dict)
#         # for idx, grad in enumerate(grads):
#         #     gradBuffer[idx] += grad
#         # feed_dict = dict(zip(gradient_holders, gradBuffer))
#         # _ = sess.run(update_batch, feed_dict=feed_dict)


# test = model()
# test.generate_sample()
# test.step(range(3))
# print(test.sample)
# print(test.total_reward)
